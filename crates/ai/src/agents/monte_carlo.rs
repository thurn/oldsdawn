// Copyright Â© Spelldawn 2021-present

// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//    https://www.apache.org/licenses/LICENSE-2.0

// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use std::collections::HashSet;
use std::f64::consts;

use actions;
use anyhow::Result;
use data::game::{GamePhase, GameState};
use data::game_actions::UserAction;
use data::primitives::Side;
use ordered_float::NotNan;
use petgraph::prelude::{EdgeRef, NodeIndex};
use petgraph::{Direction, Graph};
use rand::prelude::IteratorRandom;
use rand::thread_rng;
use with_error::{fail, WithError};

use crate::core::legal_actions;
use crate::core::types::{notnan, StatePredictionIterator};

pub fn execute(mut states: StatePredictionIterator, side: Side) -> Result<UserAction> {
    let game = states.next().with_error(|| "Expected game state")?.state;
    uct_search(&game, side, 1000)
}

type RewardValue = NotNan<f64>;

#[derive(Debug, Clone)]
struct SearchNode {
    /// Player who acted to create this node
    pub side: Side,
    /// Q(v): Total reward of all playouts that passed through this state
    pub total_reward: RewardValue,
    /// N(v): Visit count for this node    
    pub visit_count: u32,
}

struct SearchEdge {
    pub action: UserAction,
}

type SearchGraph = Graph<SearchNode, SearchEdge>;

/// Primary UCT search function.
///
/// Monte carlo tree search operates over a tree of game state nodes connected
/// by game actions. The search follows these three steps repeatedly:
///
/// 1) **Tree Policy:** Find a node in the tree which has not previously been
/// explored. The UCT algorithm provides a mathematical heuristic for how to
/// prioritize nodes to explore.
///
/// 2) **Default Policy:** Score this node to determine its reward value (âˆ†),
/// typically by playing random moves until the game terminates.
///
/// 3) **Backpropagation:** Walk back up the tree, adding the resulting reward
/// value to each parent node.
///
/// Pseudocode:
/// ```text
/// ðŸð®ð§ðœð­ð¢ð¨ð§ UCTSEARCH(sâ‚€)
///   create root node vâ‚€ with state sâ‚€
///   ð°ð¡ð¢ð¥ðž within computational budget ðð¨
///     vâ‚ â† TREEPOLICY(vâ‚€)
///     âˆ† â† DEFAULTPOLICY(s(vâ‚))
///     BACKUP(vâ‚, âˆ†)
///   ð«ðžð­ð®ð«ð§ ð’‚(BESTCHILD(vâ‚€, 0))
/// ```
pub fn uct_search(game_state: &GameState, side: Side, simulation_steps: u32) -> Result<UserAction> {
    let mut graph = SearchGraph::new();
    let root = graph.add_node(SearchNode { total_reward: notnan(0.0), visit_count: 1, side });
    for _ in 0..simulation_steps {
        let mut game = game_state.clone();
        let node = tree_policy(&mut graph, &mut game, root)?;
        let reward = default_policy(game, side)?;
        backup(&mut graph, side, node, reward)?;
    }

    let (action, _) =
        best_child(&graph, root, legal_actions::evaluate(game_state, side)?.collect(), 0.0)?;
    Ok(action)
}

/// Returns a descendant node to examine next for the provided parent node,
/// either:
///  * A node which has not yet been explored
///  * The best terminal node descendant, if all nodes have been explored.
///
/// If possible actions are available from this node which have not yet been
/// explored, selects an action and applies it, returning the result as a new
/// child. Otherwise, selects the best child to explore based on visit counts
/// and known rewards, using the [best_child] algorithm, and then repeats this
/// process recursively until an unseen node is found (or the best child is
/// terminal).
///
/// Mutates the provided [GameState] to represent the game state at the returned
/// node.
///
/// Cáµ– is the exploration constant, Cáµ– = 1/âˆš2 was suggested by Kocsis and
/// SzepesvaÌri as a good choice.
///
/// Pseudocode:
/// ```text
/// ðŸð®ð§ðœð­ð¢ð¨ð§ TREEPOLICY(v)
///   ð°ð¡ð¢ð¥ðž v is nonterminal ðð¨
///     ð¢ðŸ v not fully expanded ð­ð¡ðžð§
///       ð«ðžð­ð®ð«ð§ EXPAND(v)
///     ðžð¥ð¬ðž
///       v â† BESTCHILD(v, Cáµ–)
///   ð«ðžð­ð®ð«ð§ v
/// ```
fn tree_policy(
    graph: &mut SearchGraph,
    game: &mut GameState,
    mut node: NodeIndex,
) -> Result<NodeIndex> {
    while !matches!(game.data.phase, GamePhase::GameOver(_)) {
        let actions =
            legal_actions::evaluate(game, current_priority(game)?)?.collect::<HashSet<_>>();
        let explored = graph.edges(node).map(|e| e.weight().action).collect::<HashSet<_>>();
        if let Some(action) = actions.iter().find(|a| !explored.contains(a)) {
            // An action exists which has not yet been tried
            return expand(graph, game, node, *action);
        } else {
            // All actions have been tried, recursively search the best candidate
            let (action, best) = best_child(graph, node, actions, consts::FRAC_1_SQRT_2)?;
            actions::handle_user_action(game, current_priority(game)?, action)?;
            node = best;
        }
    }

    Ok(node)
}

/// Generates a new tree node by applying the next untried action from the
/// provided input node. Mutates the provided [GameState] to apply the
/// provided game action.
///
/// Pseudocode:
/// ```text
/// ðŸð®ð§ðœð­ð¢ð¨ð§ EXPAND(v)
///   choose ð’‚ âˆˆ untried actions from A(s(v))
///   add a new child vâ€² to v
///     with s(vâ€²) = f(s(v), ð’‚)
///     and ð’‚(vâ€²) = ð’‚
///   ð«ðžð­ð®ð«ð§ vâ€²
/// ```
fn expand(
    graph: &mut SearchGraph,
    game: &mut GameState,
    source: NodeIndex,
    action: UserAction,
) -> Result<NodeIndex> {
    let side = current_priority(game)?;
    // Instead of selecting an untried action here, we pass in the one we found in
    // `tree_policy` to avoid redundancy.
    actions::handle_user_action(game, side, action)?;
    let target = graph.add_node(SearchNode { side, total_reward: notnan(0.0), visit_count: 0 });
    graph.add_edge(source, target, SearchEdge { action });
    Ok(target)
}

/// Picks the most promising child node to explore, returning its associated
/// action and node identifier.
fn best_child(
    graph: &SearchGraph,
    node: NodeIndex,
    legal: HashSet<UserAction>,
    exploration_bias: f64,
) -> Result<(UserAction, NodeIndex)> {
    let parent_visits = graph[node].visit_count;
    let result = graph
        .edges(node)
        // We re-check action legality here because the set of legal actions can change between
        // visits, e.g. if different cards are drawn
        .filter(|edge| legal.contains(&edge.weight().action))
        .max_by_key(|edge| {
            let child = &graph[edge.target()];
            // This can technically panic when invoked from root with a very small
            // simulation count, so don't do that :)
            assert_ne!(child.visit_count, 0);
            let child_visits = f64::from(child.visit_count);
            let exploration = child.total_reward / child_visits;
            let exploitation = f64::sqrt((2.0 * f64::ln(f64::from(parent_visits))) / child_visits);
            exploration + (exploration_bias * exploitation)
        })
        .with_error(|| "No children found")?;
    Ok((result.weight().action, result.target()))
}

/// Plays out a game using random moves to determine its outcome until a
/// terminal state is reached.
///
/// Pseudocode:
/// ```text
/// ðŸð®ð§ðœð­ð¢ð¨ð§ DEFAULTPOLICY(s)
///   ð°ð¡ð¢ð¥ðž s is non-terminal ðð¨
///     choose ð’‚ âˆˆ A(s) uniformly at random
///     s â† f(s,ð’‚)
///   ð«ðžð­ð®ð«ð§ reward for state s
/// ```
fn default_policy(mut game: GameState, side: Side) -> Result<RewardValue> {
    for _ in 0..60 {
        if let GamePhase::GameOver(data) = game.data.phase {
            return Ok(notnan(if data.winner == side { 10.0 } else { -10.0 }));
        }

        let side = current_priority(&game)?;
        let action = legal_actions::evaluate(&game, side)?
            .choose(&mut thread_rng())
            .with_error(|| "No actions found")?;
        actions::handle_user_action(&mut game, side, action)?;
    }

    Ok(notnan(f64::from(game.player(side).score) - f64::from(game.player(side.opponent()).score)))
}

/// Once a playout is completed, the backpropagation step walks back up the
/// hierarchy of parent nodes, adding the resulting reward value to each one.
///
/// Pseudocode:
/// ```text
/// ðŸð®ð§ðœð­ð¢ð¨ð§ BACKUP(v,âˆ†)
///   ð°ð¡ð¢ð¥ðž v is not null ðð¨
///     N(v) â† N(v) + 1
///     Q(v) â† Q(v) + âˆ†(v, p)
///     v â† parent of v
/// ```
fn backup(
    graph: &mut SearchGraph,
    maximizing_side: Side,
    mut node: NodeIndex,
    reward: RewardValue,
) -> Result<()> {
    loop {
        let weight = graph.node_weight_mut(node).with_error(|| "Node not found")?;
        weight.visit_count += 1;
        weight.total_reward += if weight.side == maximizing_side { reward } else { -reward };

        node = match graph.neighbors_directed(node, Direction::Incoming).next() {
            Some(n) => n,
            _ => return Ok(()),
        };
    }
}

fn current_priority(game: &GameState) -> Result<Side> {
    if actions::can_take_action(game, Side::Overlord) {
        Ok(Side::Overlord)
    } else if actions::can_take_action(game, Side::Champion) {
        Ok(Side::Champion)
    } else {
        fail!("No player can take action!")
    }
}
